---
title: "Multiple Lineare Regression (Teil 2)"
author: "Peter von Rohr"
date: "25 April 2016"
output: 
  beamer_presentation:
    includes:
      in_header: tex/ethslidesheader.tex
  keep_tex: yes 
output_file: asmas_w10_v3.pdf
documentclass: ETHbeamerclass
classoption: first,ETH3,navigation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, results ='asis')
```


## Outline

- Tests und Konfidenzintervalle
- Analyse der Residuen
- Modellwahl


## Annahmen für ein lineares Modell
- Ausser, dass die Matrix $\mathbf{X}$ vollen Rang hat ($p < n$) wurden bis jetzt keine Annahmen gemacht
1. Lineares Modell ist korrekt $\rightarrow E(\mathbf{\epsilon}) = \mathbf{0}$
2. Die Werte in $\mathbf{X}$ sind exakt
3. Die Varianz der Fehler ist konstant ("Homoskedazidität") für alle Beobachtungen $\rightarrow Var(\mathbf{\epsilon}) = \mathbf{I} * \sigma^2$
4. Die Fehler sind unkorreliert
5. Weitere Eigenschaften folgen, falls die Fehler normal verteilt sind

Was passiert, wenn Annahmen nicht erfüllt sind?


## Massnahmen und Alternativen
- Falls Annahme 3 (konstante Varianzen) verletzt ist, verwenden wir weighted least squares
- Falls Annahme 5 der Normalität nicht gilt, verwenden wir robuste Methoden
- Falls Annahme 2 falsch ist, brauchen wir eine Methode namens "errors in variables"
- Falls Annahme 1 nicht zutrifft, brauchen wir ein nicht-lineares Modell


## Annahmen 1 und 4 nicht erfüllt
```{r PillKink}
rcoursetools::insertOdgAsPdf(psOdgFileStem = "PillKink", pnPaperWidthScale = 0.8, psOdgDir = "../w9/odg")
```


## Mehrere Regressionen mit einer Variablen
- __Wichtig__: Multiple lineare Regression nicht durch mehrere Regressionen mit einer Variablen ersetzen
- Beispiel: $y = 2*x1 - x2$

```{r MultiLinRegNoSimpleLinReg, echo=FALSE, results='asis'}
x1 <- c(0, 1, 2, 3, 0, 1, 2, 3)
x2 <- c(-1,0,1,2,1,2,3,4)
y <- 2*x1-x2
dfTab <- rbind(x1,x2,y)
pander::pander(dfTab, style = "rmarkdown")
```

## Einfache Regression mit x2
```{r SimpleLinRegX2, echo=TRUE}
x1 <- c(0, 1, 2, 3, 0, 1, 2, 3)
x2 <- c(-1,0,1,2,1,2,3,4)
y <- 2*x1-x2
dfData <- data.frame(x1=x1, x2=x2, y=y)
lm_simple_x2 <- lm(y ~ x2, data = dfData)
```

## Resultat
```{r SimpleLinRegX2Result, echo=FALSE, results='asis'}
pander::pander(lm_simple_x2)
```

- Original:  $y = 2*x1 - x2$


## Eigenschaften der Least Squares Schätzer
- Modell: $\mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}$, mit $E[\mathbf{\epsilon}] = \mathbf{0}$, $Cov(\mathbf{\epsilon}) = \mathbf{I}*\sigma^2$

1. $E[\hat{\mathbf{\beta}}] = \mathbf{\beta} \rightarrow$ unverzerrter Schätzer (unbiasedness) \vspace{1ex}
2. $E[\hat{\mathbf{Y}}] = E[\mathbf{Y}] = \mathbf{X}\mathbf{\beta} \rightarrow E[\mathbf{r}] = \mathbf{0}$ \vspace{1ex}
3. $Cov(\hat{\mathbf{\beta}}) = \sigma^2(\mathbf{X}^T\mathbf{X})^{-1}$ \vspace{1ex}
4. $Cov(\hat{\mathbf{Y}}) = \sigma^2 P$, $Cov(\mathbf{r}) = \sigma^2 (\mathbf{I} - \mathbf{P})$ \vspace{1ex}

wobei $\mathbf{P} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$

## Verteilung der Schätzer
Annahme, dass $\mathbf{\epsilon}$ normal-verteilt sind, daraus folgt

1. $\hat{\mathbf{\beta}} \sim \mathcal{N}_p(\mathbf{\beta}, \sigma^2(\mathbf{X}^T\mathbf{X})^{-1})$ \vspace{1ex}
2. $\hat{\mathbf{Y}} \sim \mathcal{N}_n(\mathbf{X}\mathbf{\beta}, \sigma^2 P)$ \vspace{1ex}
3. $\hat{\sigma}^2 \sim \frac{\sigma^2}{n-p}\chi^2$ \vspace{1ex}


## Tests und Vertrauensintervalle
- Angenommen, wir möchten wissen, ob eine bestimmte erklärende Variable $\beta_j$ relevant ist in unserem Modell, dann testen wir die Nullhypothese 
$$H_0: \beta_j = 0$$
gegenüber der Alternativhypothese 
$$H_A: \beta_j  \ne 0$$

- Bei unbekanntem $\sigma^2$ ergibt sich folgende Teststatistik
$$T_j = \frac{\hat{\beta}_j}{\sqrt{\hat{\sigma}^2(\mathbf{X}^T\mathbf{X})^{-1}_{jj}}} \sim t_{n-p}$$

wobei $t_{n-p}$ für die Student-t Verteilung mit $n-p$ Freiheitsgraden steht.


## Probleme bei t-Tests
- Multiples Testen bei vielen $\beta_j$, d.h. falls wir $100$ Tests mit Irrtumswahrscheinlchkeit $5\%$ machen, sind automatisch $5$ Tests signifikant
- Es kann passieren, dass für kein $\beta_j$ die Nullhypothese verworfen werden kann, aber die erklärende Variable trotzdem einen Einfluss hat. Der Grund dafür sind Korrelationen zwischen erklärenden Variablen
- Individuelle t-tests für $H_0: \beta_j = 0$ sind so zu interpretieren, dass diese den Effekt von $\beta_j$ quantifizieren nach Abzug des Einflusses aller anderen Variablen auf die Zielgrösse $Y$

$\rightarrow$ falls z. Bsp. $\beta_i$ und $\beta_j$ stark korreliert sind und wir testen die beiden Nullhypothesen $H_{0j}: \beta_j = 0$ und $H_{0i}: \beta_i = 0$, kann durch die Korrektur der anderen Variablen der Effekt von $\beta_i$ und $\beta_j$ auf $Y$ durch den t-Test nicht gefunden werden.


## Varianzanalyse und F-Test

- Zerlegung der quadrierten Abweichungen der Beobachtungen $Y$ vom Mittelwert $\bar{Y}$ in zwei Teile
1. Abweichung des gefitteten Wertes $\hat{Y}$ vom Mittelwert $\bar{Y}$
2. 
