---
title: "ASMNW - Lösung 4"
author: "Peter von Rohr"
date: "`r Sys.Date()`"
output: pdf_document
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Kontrollfrage 1
Eine Voraussetzung für die Verwendung von Least Squares zur Schätzung der Parameter ist, dass die Matrix $\mathbf{X}$ vollen Kolonnenrang hat. Welche Beziehung resultiert aus dieser Voraussetzung für die Beziehung zwischen den Anzahl Parametern $p$ und die Anzahl Beobachtungen $n$?

### Lösung
Es muss gelten: $p < n$, d.h. die Anzahl Parameter muss kleiner sein als die Anzahl Beobachtungen


## Kontrollfrage 2
Abgesehen von Kolonnenrang, wie lauten die fünf weiteren Bedingungen für das Verwenden von multiplen linearen Regressionen

### Lösung
1. Lineares Modell ist korrekt $\rightarrow E(\mathbf{\epsilon}) = \mathbf{0}$
2. Die Werte in $\mathbf{X}$ sind exakt
3. Die Varianz der Fehler ist konstant ("Homoskedazidität") für alle Beobachtungen $\rightarrow Var(\mathbf{\epsilon}) = \mathbf{I} * \sigma^2$
4. Die Fehler sind unkorreliert
5. Weitere Eigenschaften folgen, falls die Fehler normal verteilt sind


## Kontrollfrage 3
Falls die Bedingung der konstanten Varianz nicht erfüllt ist, hatten wir gezeigt, dass __Generalised Least Squares__ verwendet werden kann. Bei generalised least square nehmen wir an, dass 

$$var(\mathbf{\epsilon}) = \Sigma$$

Diese Co-Varianzmatrix $\Sigma$ wird mit der Cholesky-Zerlegung und das Produkt $\Sigma = \mathbf{C}\mathbf{C}^T$ zerlegt. Die Zielgrössen $\mathbf{y}$ und die erklärenden Variablen in $\mathbf{X}$ werden mit der Matrix $C^{-1}$ transformiert. Daraus resultieren dann

\begin{eqnarray}
\tilde{\mathbf{y}} &=& \mathbf{C}^{-1} \mathbf{y} \nonumber \\
\tilde{\mathbf{X}} &=& \mathbf{C}^{-1} \mathbf{X}
\label{eq:TildeTransform}
\end{eqnarray}

Gegeben unser ursprüngliches lineares Modell


\begin{equation}
\mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}
\label{eq:LinearModel}
\end{equation}

Wie sieht die Beziehung zwischen den Grössen $\tilde{\mathbf{y}}$ und $\tilde{\mathbf{X}}$ aus, insbesondere handelt es sich bei dieser Beziehung wieder um ein lineares Modell und wenn ja, wie sieht dieses aus?

### Lösung
Aufgrund der Transformationen in Gleichung (\ref{eq:TildeTransform}) folgt, dass 

\begin{eqnarray}
\mathbf{C} * \tilde{\mathbf{y}} &=& \mathbf{y} \nonumber \\
\mathbf{C} * \tilde{\mathbf{X}} &=&  \mathbf{X}
\label{eq:TildeBackTransform}
\end{eqnarray}

Setzen wir die Beziehungen aus Gleichung (\ref{eq:TildeBackTransform}) in unser lineares Modell (\ref{eq:LinearModel}) ein, dann erhalten wir

\begin{equation}
\mathbf{C} * \tilde{\mathbf{y}} = \mathbf{C} * \tilde{\mathbf{X}}\mathbf{\beta} + \mathbf{\epsilon}
\label{eq:LinearModelInserted}
\end{equation}

Durch Multiplikation von links beider Seiten in Gleichung (\ref{eq:LinearModelInserted}) mit $\mathbf{C}^{-1}$ erhalten wir

\begin{equation}
\tilde{\mathbf{y}} = \tilde{\mathbf{X}}\mathbf{\beta} + \mathbf{C}^{-1}\mathbf{\epsilon}
\label{eq:LinearModelTilde}
\end{equation}

Ersetzen wir analog zu $\mathbf{y}$ und $\mathbf{X}$, $\mathbf{C}^{-1}\mathbf{\epsilon}$ durch $\tilde{\mathbf{\epsilon}}$, dann resultiert wieder ein lineares Modell der Form:

$$\tilde{\mathbf{y}} = \tilde{\mathbf{X}}\mathbf{\beta} +  \tilde{\mathbf{\epsilon}}$$



## Aufgabe 1
Für unseren Datensatz aus der Vorlesung mit den Zunahmen und dem Geburtsgewicht (siehe \url{http://charlotte-ngs.github.io/GELASM/w12/gain_data.csv}) nehmen wir an, dass die Reste $\mathbf{\epsilon}$ nicht mehr konstant und unkorreliert sind, sondern dass folgende Covarianzmatrix gefunden wurde.


```{r GenerateRandCovMat, echo=FALSE, results='hide'}
set.seed(7643)
nNrCalf <- 5
Calf <- 1:nNrCalf
WWG <- c(4.5, 2.9, 3.9, 3.5, 5.0)/5
PWG <- c(6.8, 5.0, 6.8, 6.0, 7.5)/5
BW <- round(25*PWG + rnorm(nNrCalf), digits = 1)
dfGainData <- data.frame(Calf, BW, WWG=WWG, PWG=PWG)

mRandC <- matrix(round(runif(nNrCalf*nNrCalf)+1, digits = 1), ncol = nNrCalf)
mRandC[upper.tri(mRandC)] <- 0
mRandC[lower.tri(mRandC)] <- mRandC[lower.tri(mRandC)] / 20
mSigma <- round(mRandC %*% t(mRandC), digits = 2)
write.csv2(mSigma, file="covar_sigma.csv", row.names = FALSE, quote = FALSE)
```

$$\Sigma = \left[
  \begin{array}{rrrrr}
      1.44 & 0.12 & 0.08 & 0.09 & 0.12 \\ 
      0.12 & 2.26 & 0.12 & 0.10 & 0.09 \\ 
      0.08 & 0.12 & 3.25 & 0.18 & 0.16 \\ 
      0.09 & 0.10 & 0.18 & 2.91 & 0.14 \\ 
      0.12 & 0.09 & 0.16 & 0.14 & 2.27
  \end{array}
\right]
$$

Die Matrix ist verfügbar als CSV-Datei unter: \url{http://charlotte-ngs.github.io/GELASM/w12/covar_sigma.csv} verfügbar. 

### Ihre Aufgabe
Schätzen sie die Regressionskoeffizienten unter Berücksichtigung der Covarianzmatrix $\Sigma$.

### Hinweise
Die Funktion `chol()` macht die Cholesky-Faktorisierung in R. Das Resultat von `chol()` ist eine obere rechte Dreiecksmatrix. Dies entspricht der Matrix $\mathbf{C}^T$ in der Vorlesung. Die Inverse einer Matrix lässt sich mit der Funktion `solve()` berechnen.

### Lösung
Als erstes lesen wir die Daten und die Varianz-Kovarianzmatrix von den entsprechenden CSV-Files ein. Das Dataframe mit der Covarianzmatrix konvertieren wir in eine Matrix. 

```{r ReadCovSigma}
dfGainData <- read.csv2(file = "http://charlotte-ngs.github.io/GELASM/w12/gain_data.csv", 
                        stringsAsFactors = FALSE)
dfCovSigma <- read.csv2(file = "http://charlotte-ngs.github.io/GELASM/w12/covar_sigma.csv", 
                        stringsAsFactors = FALSE)
mCovSigma <- as.matrix(dfCovSigma)
```

Die Covarianzmatrix wird anhand der Cholesky-Faktorisierung zerlegt.

```{r CholCovSigma}
matC <- t(chol(mCovSigma))
```

Die Inverse von `matC` wird verwendet um die Zielgrösse `PWG` und die erklärenden Variablen zu transformieren. 

```{r MatCInvTrans}
matCInv <- solve(matC)
vPwgTilde <- matCInv %*% dfGainData[,"PWG"]
vWwgTilde <- matCInv %*% dfGainData[,"WWG"]
vBwTilde <- matCInv %*% dfGainData[,"BW"]
```

Mit den transformierten Grössen können wir das lineare Modell anpassen

```{r LinModTilde}
dfGainTilde <- data.frame(PWGTilde = vPwgTilde, WWGTilde = vWwgTilde, BWTilde = vBwTilde)
summary(lmGainTilde <- lm(PWGTilde ~ ., data = dfGainTilde))
```

